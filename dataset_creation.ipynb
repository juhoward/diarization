{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "The goal is to fine-tune a model that can predict an appropiate response, given a conversation of a given length.\n",
    "\n",
    "The context the model will use can vary within practical limits. The model should be able to successfully respond during different phases of a digital optometrics remote exam.\n",
    "\n",
    "### Stages of an exam:\n",
    "- #### Introduction\n",
    "  - Initialization: a greeting\n",
    "  - During: local technician may need to set the patient up. Assistant asks the name of the patient and introduces itself.\n",
    "  - Transition: the local technician or patient will indicate their readiness to proceed. The assistant should explain the visual acuity exam\n",
    " ####\n",
    "- #### Visual Acuity\n",
    "  - Initialization: ready signal from local tech or patient.\n",
    "  - During: give instructions to the patient to assess the visual acuity without a prescription applied.\n",
    "  - Transition: binocular and monocular visual acuity for both eyes is observed.\n",
    " ####\n",
    "- #### Subjective Refraction\n",
    "  - Initialization: monocular visual acuity without the prescription is completed.\n",
    "  - During: give instructions that facilitate monocular, then binocular lens comparisons. Binocular lens comparisons are complete when patient indicates no difference when lenses are changed. \n",
    "  - Transition: Patient is shown previous prescription and newly found prescription and asked if they see a difference.\n",
    " ####\n",
    "- #### Close Vision Test\n",
    "  - Initialization: Patient is shown a comparison of old and new prescriptions. Local tech is instructed to place reading card in front of patient. Ready signal should be given\n",
    "  - During: Instructions are given to the patient to read a specific row of the card. If patient correctly identifies 80% of the letters correctly, the exam ends.\n",
    "  - Transition: Exit instructions and valediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "data_dir = './recordings/'\n",
    "jjara = data_dir + 'RT-JJara/'\n",
    "stephens = data_dir + 'rt-lstephens/'\n",
    "brokus = data_dir + 'rt-sbrokus/'\n",
    "# take only first 10 of jjara, listed in some weird order in the directory\n",
    "good_files = [\n",
    "    'recordings/RT-JJara/17885_428_VC_1_1_28_11_2023_15_20_01',\n",
    "    'recordings/RT-JJara/28081_391_VC_1_1_28_11_2023_17_27_15',\n",
    "    'recordings/RT-JJara/34401_575_VC_1_1_28_11_2023_14_42_16',\n",
    "    'recordings/RT-JJara/73015_939_VC_1_1_28_11_2023_15_22_48',\n",
    "    'recordings/RT-JJara/102725_1374_VC_1_1_04_12_2023_16_44_06',\n",
    "    'recordings/RT-JJara/298880_387_VC_1_1_04_12_2023_15_43_49',\n",
    "    'recordings/RT-JJara/469703_9570_VC_1_1_29_11_2023_16_23_53',\n",
    "    'recordings/RT-JJara/477169_1508_VC_1_1_30_11_2023_16_26_11',\n",
    "    'recordings/RT-JJara/521679_1570_VC_1_1_30_11_2023_12_05_38',\n",
    "    'recordings/RT-JJara/595841_4602_VC_1_1_01_12_2023_14_34_27'\n",
    "]\n",
    "jjara = []\n",
    "for f in good_files:\n",
    "    jjara.append('./' + f + '/clean_captions1.txt')\n",
    "# other directories are cleaner.\n",
    "stephens = glob(stephens + '*/clean_captions1.txt')\n",
    "brokus = glob(brokus + '*/clean_captions1.txt')\n",
    "for i in [jjara, stephens, brokus]:\n",
    "    print(len(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Selection\n",
    "\n",
    "there are 30 diaries containing full vision exams. To form a dataset, we will create dialogues of random length between 1 and 10 lines of dialogue.\n",
    "\n",
    "Each selection should end with the assistant's predicted response.\n",
    "\n",
    "Step 1 - split caption files into train/val/split \n",
    "\n",
    "Step 2 - randomly select dialogue length, from 1 to 10 lines\n",
    "\n",
    "Step 3 - select lines from a random file. Check to make sure the last line is the assistant.\n",
    "\n",
    "Step 4 - separate the dialogue from the assistant's response.\n",
    "\n",
    "## Step 1\n",
    "\n",
    "We have 30 files to be divided in a 70/10/20 train/val/test split.\n",
    "\n",
    "- 21 will randomly be chosen to be in the training set. \n",
    "\n",
    "- 3 for the validation set. \n",
    "\n",
    "- 6 for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "['./recordings/RT-JJara/521679_1570_VC_1_1_30_11_2023_12_05_38/clean_captions1.txt', './recordings/RT-JJara/595841_4602_VC_1_1_01_12_2023_14_34_27/clean_captions1.txt', './recordings/rt-lstephens/1442410_6967_VC_1_1_01_12_2023_18_35_50/clean_captions1.txt']\n",
      "['./recordings/rt-lstephens/1052783_6838_VC_1_1_04_12_2023_15_49_25/clean_captions1.txt', './recordings/rt-lstephens/612446_5307_VC_1_1_01_12_2023_18_03_36/clean_captions1.txt', './recordings/rt-sbrokus/34898_571_VC_1_1_04_12_2023_11_05_51/clean_captions1.txt']\n",
      "['./recordings/rt-sbrokus/405969_4506_VC_1_1_01_12_2023_14_38_33/clean_captions1.txt', './recordings/rt-sbrokus/566198_1378_VC_1_1_28_11_2023_16_38_04/clean_captions1.txt']\n"
     ]
    }
   ],
   "source": [
    "all_files  = jjara.copy()\n",
    "all_files  += stephens\n",
    "all_files  += brokus\n",
    "\n",
    "print(len(all_files))\n",
    "print(all_files[8:11])\n",
    "print(all_files[18:21])\n",
    "print(all_files[28:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/digitalopt/proj/datasets/Exam_v1/test/000024.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/digitalopt/proj/datasets/Exam_v1/train/000003.txt\n",
      "before cleaning:\n",
      "['[SPEAKER_00]  Hello.', '[SPEAKER_01]  Hi, how are you two doing today?', '[SPEAKER_00]  Fine. Have a seat in that chair right there.', '[SPEAKER_01]  Great. Is [NAME] with us today?', '[SPEAKER_03]  I am.']\n",
      "Dialogue contains : 4 speakers.\n",
      "speaker: SPEAKER_00, words spoken: 68\n",
      "speaker: SPEAKER_01, words spoken: 516\n",
      "speaker: SPEAKER_03, words spoken: 111\n",
      "speaker: SPEAKER_02, words spoken: 30\n",
      "More than three speakers in captions! Only 3 speakers allowed.\n",
      "sorted speakers by word count: \n",
      "[('SPEAKER_02', 30), ('SPEAKER_00', 68), ('SPEAKER_03', 111), ('SPEAKER_01', 516)]\n",
      "/home/digitalopt/proj/datasets/Exam_v1/train/000003.txt is invalid! Only 3 speakers allowed. Removing speakers by lowest word count.\n",
      "truncated speaker list: ['SPEAKER_00', 'SPEAKER_03', 'SPEAKER_01']\n",
      "speaker mapping <spaker in data> : <new spaker label>\n",
      "{'SPEAKER_00': 'LocalTech', 'SPEAKER_03': 'Patient__', 'SPEAKER_01': 'Assistant'}\n",
      "After:\n",
      "['[LocalTech]  Hello.', '[Assistant]  Hi, how are you two doing today?', '[LocalTech]  Fine. Have a seat in that chair right there.', '[Assistant]  Great. Is [NAME] with us today?', '[Patient__]  I am.']\n"
     ]
    }
   ],
   "source": [
    "# found that some data files still contain SPEAKER_XX speaker name format\n",
    "from diarization_utils import CaptionCleaner\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "cleaner = CaptionCleaner()\n",
    "data_dir = '/home/digitalopt/proj/datasets/Exam_v1/'\n",
    "all_files = glob(data_dir + '*/*.txt')\n",
    "all_files.sort()\n",
    "for p in all_files:\n",
    "    captions = cleaner.read_captions(p)\n",
    "    if captions[0].startswith(\"[SPEAKER_\"):\n",
    "        print(f'{p}')\n",
    "        print(f'before cleaning:\\n{captions[:5]}')\n",
    "        cleaner.speaker_cnt(captions)\n",
    "        new = cleaner.remap_speaker_names(captions)\n",
    "        print(f'After:\\n{new[:5]}')\n",
    "        # cleaner.write_captions(new, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 21\tval: 3\t \ttrainval: 24\ttest: 6\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "total = 30\n",
    "splits = [21, 3, 6]\n",
    "# training set indices\n",
    "sample = random.sample(range(0,total-1), splits[0])\n",
    "total -= splits[0]\n",
    "# check for duplicates\n",
    "train_idx = set(sample)\n",
    "while len(train_idx) < splits[0]:\n",
    "    val = random.randint(0, total - 1)\n",
    "    train_idx.add(val)\n",
    "\n",
    "# validation set\n",
    "val_idx = set()\n",
    "while len(val_idx) < splits[1]:\n",
    "    val = random.randint(0, total - 1)\n",
    "    if val in train_idx:\n",
    "        continue\n",
    "    else:\n",
    "        val_idx.add(val)\n",
    "test_idx = [i for i in range(30)]\n",
    "test_idx = set(test_idx)\n",
    "trainVal = train_idx.union(val_idx)\n",
    "test_idx = test_idx - trainVal\n",
    "print(f'train: {len(train_idx)}\\tval: {len(val_idx)}\\t \\ttrainval: {len(trainVal)}\\ttest: {len(test_idx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train set ...\n",
      "Building val set ...\n",
      "Building test set ...\n",
      "train 21\n",
      "val 3\n",
      "test 6\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "datasets = ['train', 'val','test']\n",
    "file_paths = defaultdict(list)\n",
    "\n",
    "indices= [train_idx, val_idx, test_idx]\n",
    "for idx, idx_group in enumerate(indices):\n",
    "    print(f'Building {datasets[idx]} set ...')\n",
    "    for n in idx_group:\n",
    "        file_paths[datasets[idx]].append(all_files[n])\n",
    "\n",
    "for (k,v) in file_paths.items():\n",
    "    print(k + ' ' + str(len(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./recordings/RT-JJara/28081_391_VC_1_1_28_11_2023_17_27_15/clean_captions1.txt',\n",
       " './recordings/RT-JJara/34401_575_VC_1_1_28_11_2023_14_42_16/clean_captions1.txt',\n",
       " './recordings/RT-JJara/298880_387_VC_1_1_04_12_2023_15_43_49/clean_captions1.txt',\n",
       " './recordings/RT-JJara/469703_9570_VC_1_1_29_11_2023_16_23_53/clean_captions1.txt',\n",
       " './recordings/RT-JJara/477169_1508_VC_1_1_30_11_2023_16_26_11/clean_captions1.txt',\n",
       " './recordings/RT-JJara/521679_1570_VC_1_1_30_11_2023_12_05_38/clean_captions1.txt',\n",
       " './recordings/RT-JJara/595841_4602_VC_1_1_01_12_2023_14_34_27/clean_captions1.txt',\n",
       " './recordings/rt-lstephens/1442410_6967_VC_1_1_01_12_2023_18_35_50/clean_captions1.txt',\n",
       " './recordings/rt-lstephens/696884_5310_VC_1_1_05_12_2023_11_16_56/clean_captions1.txt',\n",
       " './recordings/rt-lstephens/1711515_11357_VC_1_1_28_11_2023_18_25_35/clean_captions1.txt',\n",
       " './recordings/rt-lstephens/973397_4715_VC_1_1_30_11_2023_17_22_09/clean_captions1.txt',\n",
       " './recordings/rt-lstephens/1130238_6039_VC_1_1_01_12_2023_16_39_52/clean_captions1.txt',\n",
       " './recordings/rt-lstephens/159838_1444_VC_1_1_01_12_2023_10_13_17/clean_captions1.txt',\n",
       " './recordings/rt-lstephens/1052783_6838_VC_1_1_04_12_2023_15_49_25/clean_captions1.txt',\n",
       " './recordings/rt-sbrokus/34898_571_VC_1_1_04_12_2023_11_05_51/clean_captions1.txt',\n",
       " './recordings/rt-sbrokus/359862_4466_VC_1_1_04_12_2023_13_36_20/clean_captions1.txt',\n",
       " './recordings/rt-sbrokus/95892_1244_VC_1_1_01_12_2023_15_27_20/clean_captions1.txt',\n",
       " './recordings/rt-sbrokus/163548_1814_VC_1_1_04_12_2023_12_07_16/clean_captions1.txt',\n",
       " './recordings/rt-sbrokus/609869_5088_VC_1_1_04_12_2023_10_34_29/clean_captions1.txt',\n",
       " './recordings/rt-sbrokus/590038_4757_VC_1_1_01_12_2023_09_21_54/clean_captions1.txt',\n",
       " './recordings/rt-sbrokus/350602_785_VC_1_1_01_12_2023_15_15_57/clean_captions1.txt']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "data_dir = '/home/digitalopt/proj/datasets/Exam_v1/'\n",
    "for d in datasets:\n",
    "    os.makedirs(data_dir + d, exist_ok=True)\n",
    "\n",
    "cnt = -1\n",
    "for k in file_paths.keys():\n",
    "    for path in file_paths[k]:\n",
    "        cnt += 1\n",
    "        newpath =  data_dir + k + '/' + str(cnt).zfill(6) + '.txt'\n",
    "        shutil.copyfile(path, newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Randomly sample 1-10 lines from texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 21\tval: 3\ttest: 6\n"
     ]
    }
   ],
   "source": [
    "from vision_dataset import VisionDataset\n",
    "from glob import glob\n",
    "\n",
    "data_dir = '/home/digitalopt/proj/datasets/Exam_v1/'\n",
    "train = glob(data_dir + 'train/*.txt')\n",
    "val = glob(data_dir + 'val/*.txt')\n",
    "test = glob(data_dir + 'test/*.txt')\n",
    "print(f'train: {len(train)}\\tval: {len(val)}\\ttest: {len(test)}')\n",
    "dataset = VisionDataset()\n",
    "captions = dataset.read_captions(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Assistant]  Hello. Can you hear me?',\n",
       " '[LocalTech]  I can, yes. We have [NAME].',\n",
       " \"[Assistant]  My name's [NAME]. I'll help you through some vision testing before the doctor sees you. [NAME]'s gonna get you lined up before we start though. Thank you, [NAME].\",\n",
       " \"[LocalTech]  Thank you. All right, let's move this a little bit. Okay. Left side. Can you get out a little  Right there is perfect. Other side. Can you move it in, please.  Well, right there. Perfect. All set.  All right.\",\n",
       " '[Assistant]  We are going to start without prescriptions first. It will look blurry, but without squinting what is the smallest row that you can read?',\n",
       " '[Patient__]  V-C-K-N-O.',\n",
       " '[Assistant]  If we go a little smaller, what is the lowest row you can see now?',\n",
       " '[Patient__]  Nothing.',\n",
       " '[Assistant]  None of those. Okay. How about any of these?',\n",
       " \"[Patient__]  Um, they're all so blurry.  Uh, D V O H C.\"]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[0].startswith(\"[Assistant]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, randrange\n",
    "\n",
    "def select_sample(captions):\n",
    "    cap_len = len(captions)\n",
    "    # randomly select starting point in data\n",
    "    start = randrange(cap_len)\n",
    "    # random end point that won't result in an empty sample\n",
    "    end = start + randint(2,10)\n",
    "    # assure sample isn't out of range\n",
    "    while end > (cap_len-1):\n",
    "        start -= randint(2,10)\n",
    "        end = start + randint(2,10)\n",
    "    # assure sample ends with Assistant response\n",
    "    while captions[(end-1)][1:10] != 'Assistant':\n",
    "        if start != 0:\n",
    "            start -= 1\n",
    "            end -= 1\n",
    "        elif start != cap_len-1:\n",
    "            start += 1\n",
    "            end += 1\n",
    "    return captions[start:end]\n",
    "# run a check for errors\n",
    "for i in range(1000):\n",
    "    sample = select_sample(captions)\n",
    "    if len(sample) < 2:\n",
    "        print(f'Bad sample: \\n{sample}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Separate dialogue from Assistant response.\n",
    "\n",
    "Also, I will try using a text formatting function to see how that performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Use the dialogue below to create a response that could help guide a patient through a vision exam.\n",
      "\n",
      "### Input:\n",
      "[{'role': 'Assistant', 'content': 'One.  Two.'}, {'role': 'Patient__', 'content': 'same'}, {'role': 'Assistant', 'content': 'Which one has clear letters?  The red side or the green side?'}, {'role': 'Patient__', 'content': 'the green side.'}, {'role': 'Assistant', 'content': 'Alright. Can you read the bottom row for me now?'}, {'role': 'Patient__', 'content': \"H, Z, can't tell if it's a C or an O, K, O.\"}]\n",
      "\n",
      "### Response:\n",
      "[{'role': 'Assistant', 'content': \"Beautiful.  Both eyes are uncovered for this comparison.  This is number one. This is number two.  Number two is the new glasses prescription.  One is the prescription.  All right.  That's our distance.  Let's show you up close.\"}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = dataset.select_sample(captions)\n",
    "\n",
    "input = dataset.to_dialogue(sample)\n",
    "\n",
    "def format_instruction(input):\n",
    "    return f'''### Instruction:\n",
    "Use the dialogue below to create a response that could help guide a patient through a vision exam.\n",
    "\n",
    "### Input:\n",
    "{input['dialogue']}\n",
    "\n",
    "### Response:\n",
    "{input['response']}\n",
    "'''\n",
    "\n",
    "print(format_instruction(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Step: create the dataset\n",
    "\n",
    "## Sampling based on exam phase\n",
    "\n",
    "    Each file is first divided into 4 sections:\n",
    "    gs - greeting & setup\n",
    "    va - visual acuity\n",
    "    sr - subjective refraction\n",
    "    cv - close vision & valediction\n",
    "    The average percentage that each exam phase, based on 30 dialogue samples\n",
    "    is used to drive the sampling of each text document so that dataset samples\n",
    "    are more consistent with the average real exam.\n",
    "    The average percentage of the total dialogue that each phase represents is as follows:\n",
    "    gs : 15%\n",
    "    va : 24%\n",
    "    sr : 54%\n",
    "    cv : 11%\n",
    "The close vision (cv) section was increased beyond it's representation in the dialogue samples to prevent the lack of sampling in dialogues that are short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 files found. Sampling 25 times per file.\n",
      "sampling file: /data/datasets/Exam_v2/train/000000.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000001.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000002.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000003.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000004.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000005.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000006.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000007.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000008.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000009.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000010.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000011.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000012.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000013.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000014.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000015.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000016.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000017.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000018.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000019.txt\n",
      "sampling file: /data/datasets/Exam_v2/train/000020.txt\n",
      "Expected length of data: 525\n",
      "Actual length: 525\n",
      "3 files found. Sampling 25 times per file.\n",
      "sampling file: /data/datasets/Exam_v2/val/000021.txt\n",
      "sampling file: /data/datasets/Exam_v2/val/000022.txt\n",
      "sampling file: /data/datasets/Exam_v2/val/000023.txt\n",
      "Expected length of data: 75\n",
      "Actual length: 75\n",
      "6 files found. Sampling 25 times per file.\n",
      "sampling file: /data/datasets/Exam_v2/test/000024.txt\n",
      "sampling file: /data/datasets/Exam_v2/test/000025.txt\n",
      "sampling file: /data/datasets/Exam_v2/test/000026.txt\n",
      "sampling file: /data/datasets/Exam_v2/test/000027.txt\n",
      "sampling file: /data/datasets/Exam_v2/test/000028.txt\n",
      "sampling file: /data/datasets/Exam_v2/test/000029.txt\n",
      "Expected length of data: 150\n",
      "Actual length: 150\n",
      "\n",
      " train 525\n",
      "\n",
      " val 75\n",
      "\n",
      " test 150\n"
     ]
    }
   ],
   "source": [
    "from vision_dataset import VisionDatasetCreator\n",
    "# avg percentages of exam phase lengths\n",
    "gs=0.15\n",
    "va=0.25\n",
    "sr=0.50\n",
    "cv=0.11\n",
    "# minumum dialogue lengths\n",
    "gs_min = 2\n",
    "va_min = 3\n",
    "sr_min = 4\n",
    "cv_min = 3\n",
    "# maximum dialogue lengths\n",
    "gs_max = 5\n",
    "va_max = 10\n",
    "sr_max = 10\n",
    "cv_max = 6\n",
    "\n",
    "sampling_strategy = dict(\n",
    "    gs=[gs, gs_min, gs_max],\n",
    "    va=[va, va_min, va_max],\n",
    "    sr=[sr, sr_min, sr_max],\n",
    "    cv=[cv, cv_min, cv_max]\n",
    ")\n",
    "\n",
    "data_dir = '/data/datasets/Exam_v2/'\n",
    "# set seed to get randomization with reproducible results\n",
    "dataset = VisionDatasetCreator(sampling_strategy, seed=42)\n",
    "# 25 samples from each file in the training set, which has 21 files total\n",
    "size = (21*25)\n",
    "dataset.load(data_dir, 'train', size)\n",
    "# 25 samples from each validation file, 3 files total\n",
    "size = (3*25)\n",
    "dataset.load(data_dir, 'val', size)\n",
    "# 25 samples from each test file, 6 files total\n",
    "size = (6*25)\n",
    "dataset.load(data_dir, 'test', size)\n",
    "\n",
    "for i in ['train', 'val', 'test']:\n",
    "    print('\\n', i, len(dataset.dataset[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "val\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "big = 0\n",
    "for k in sampling_strategy.keys():\n",
    "    if big < sampling_strategy[k][2]:\n",
    "        big = sampling_strategy[k][2]\n",
    "for name in ['train', 'val', 'test']:\n",
    "    print(name)\n",
    "    for d in dataset.dataset[name]:\n",
    "        if len(d) > big:\n",
    "            print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
