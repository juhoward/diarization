{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Llama2-7b\n",
    "\n",
    "Training a lightweight model on the visiondataset we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install \"transformers==4.34.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.23.0\" \"bitsandbytes==0.41.1\" \"trl==0.4.7\" \"safetensors>=0.3.1\" \"scipy==1.11.4\" --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for semantic similarity metrics\n",
    "!python -m pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()\n",
    "\n",
    "!python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\"\n",
    "!python -m pip install ninja packaging\n",
    "!MAX_JOBS=16\n",
    "!python -m pip install flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "14\n",
      "18\n",
      "0\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "gs_ratio=0.15\n",
    "va_ratio=0.30\n",
    "sr_ratio=0.40\n",
    "cv_ratio=0.15\n",
    "\n",
    "from math import floor, ceil\n",
    "from random import randint\n",
    "samp_size = 100\n",
    "gs_rng = floor(gs_ratio * samp_size)\n",
    "va_rng = ceil(va_ratio*samp_size)\n",
    "sr_rng = ceil(sr_ratio*samp_size)\n",
    "cv_rng = floor(cv_ratio*samp_size)\n",
    "print(gs_rng + va_rng + sr_rng + cv_rng == samp_size)\n",
    "\n",
    "gs_samps = []\n",
    "va_samps = []\n",
    "sr_samps = []\n",
    "cv_samps = []\n",
    "samps = [gs_samps, va_samps, sr_samps, cv_samps]\n",
    "\n",
    "for i in range(samp_size):\n",
    "    num = randint(1,samp_size)\n",
    "    if num < gs_rng:\n",
    "        gs_samps.append(num)\n",
    "    if num > gs_rng and num < va_rng:\n",
    "        va_samps.append(num)\n",
    "    if num > va_rng and num < cv_rng:\n",
    "        sr_samps.append(num)\n",
    "    if num < sr_rng:\n",
    "        cv_samps.append(num)\n",
    "\n",
    "for s in samps:\n",
    "    print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 files found. Sampling 15 times per file.\n",
      "sampling file: /data/datasets/Exam_v3/train/000000.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000001.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000002.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000003.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000004.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000005.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000006.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000007.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000008.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000009.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000010.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000011.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000012.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000013.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000014.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000015.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000016.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000017.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000018.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000019.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000020.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000021.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000022.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000023.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000024.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000025.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000026.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000027.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000028.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000029.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000031.txt\n",
      "sampling file: /data/datasets/Exam_v3/train/000032.txt\n",
      "Expected length of data: 480\n",
      "Actual length: 480\n",
      "5 files found. Sampling 15 times per file.\n",
      "sampling file: /data/datasets/Exam_v3/val/000034.txt\n",
      "sampling file: /data/datasets/Exam_v3/val/000035.txt\n",
      "sampling file: /data/datasets/Exam_v3/val/000037.txt\n",
      "sampling file: /data/datasets/Exam_v3/val/000038.txt\n",
      "sampling file: /data/datasets/Exam_v3/val/000040.txt\n",
      "Expected length of data: 75\n",
      "Actual length: 75\n",
      "8 files found. Sampling 15 times per file.\n",
      "sampling file: /data/datasets/Exam_v3/test/000030.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000033.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000036.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000039.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000041.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000042.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000043.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000044.txt\n",
      "Expected length of data: 120\n",
      "Actual length: 120\n",
      "\n",
      " train 480\n",
      "\n",
      " val 75\n",
      "\n",
      " test 120\n"
     ]
    }
   ],
   "source": [
    "from vision_dataset import VisionDatasetCreator, VisionDataset\n",
    "\n",
    "# avg percentages of exam phase lengths\n",
    "gs_len=0.14\n",
    "sp_len=0.25\n",
    "ac_len=0.50\n",
    "cv_len=0.11\n",
    "# percentage of samples taken from each exam phase\n",
    "gs_ratio=0.15\n",
    "sp_ratio=0.30\n",
    "ac_ratio=0.40\n",
    "cv_ratio=0.15\n",
    "# minumum dialogue lengths\n",
    "gs_min = 2\n",
    "sp_min = 4\n",
    "ac_min = 8\n",
    "cv_min = 6\n",
    "# maximum dialogue lengths\n",
    "gs_max = 5\n",
    "sp_max = 14\n",
    "ac_max = 18\n",
    "cv_max = 10\n",
    "\n",
    "sampling_strategy = dict(\n",
    "    gs=[gs_len, gs_ratio, gs_min, gs_max],\n",
    "    sp=[sp_len, sp_ratio, sp_min, sp_max],\n",
    "    ac=[ac_len, ac_ratio, ac_min, ac_max],\n",
    "    cv=[cv_len, cv_ratio, cv_min, cv_max]\n",
    ")\n",
    "\n",
    "data_dir = '/data/datasets/Exam_v3/'\n",
    "# set seed to get randomization with reprducible results\n",
    "dataset_creator = VisionDatasetCreator(sampling_strategy, seed=42)\n",
    "\n",
    "# identify the number of samples from each file in the training set, which has 21 files total\n",
    "samples = 15\n",
    "# 25 samples from each file in the training set, which has 21 files total\n",
    "size = (32*samples)\n",
    "dataset_creator.load(data_dir, 'train', size)\n",
    "# 25 samples from each validation file, 3 files total\n",
    "size = (5*samples)\n",
    "dataset_creator.load(data_dir, 'val', size)\n",
    "# 25 samples from each test file, 6 files total\n",
    "size = (8*samples)\n",
    "dataset_creator.load(data_dir, 'test', size)\n",
    "\n",
    "for i in ['train', 'val', 'test']:\n",
    "    print('\\n', i, len(dataset_creator.dataset[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length: 480  max sequence length: 1338\n",
      "dataset length: 75  max sequence length: 1053\n",
      "dataset length: 120  max sequence length: 1423\n"
     ]
    }
   ],
   "source": [
    "def format_mixtral(data, model_input=True):\n",
    "    instruction = 'You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.'\n",
    "    dialogue_string = ''\n",
    "    for i in data['dialogue']:\n",
    "        dialogue_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "    response_string = ''\n",
    "    for i in data['response']:\n",
    "        response_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "    if model_input:\n",
    "        return f'''<s>[INST]{instruction}\\n{dialogue_string}[/INST]{response_string}</s>'''\n",
    "    else:\n",
    "        return f'''<s>[INST]{instruction}\\n{dialogue_string}[/INST]'''\n",
    "\n",
    "def format_inference(data, model_input=True):\n",
    "    instruction = 'You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.'\n",
    "    dialogue_string = ''\n",
    "    for i in data['dialogue']:\n",
    "        dialogue_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "    if model_input:\n",
    "        return f'''### Instruction:\\n{instruction}\\n### Input:\\n{dialogue_string}\\n### Response:\\n'''\n",
    "    else:\n",
    "        response_string = ''\n",
    "        for i in data['response']:\n",
    "            response_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "        return f'''### Response:\\n{response_string}'''\n",
    "\n",
    "train = VisionDataset(dataset_creator.dataset[\"train\"])\n",
    "val = VisionDataset(dataset_creator.dataset[\"val\"])\n",
    "test = VisionDataset(dataset_creator.dataset[\"test\"])\n",
    "\n",
    "for i in [train, val, test]:\n",
    "    print(f'dataset length: {i.__len__()}  max sequence length: {i.get_max_len(format_inference)}')\n",
    "train_len = str(train.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue': [{'role': 'LocalTech',\n",
       "   'content': \"Alright, and I'm gonna actually, let him look at me, my nose specifically.\"},\n",
       "  {'role': 'LocalTech', 'content': 'And can her right PD come in, please?'}],\n",
       " 'response': [{'role': 'Assistant',\n",
       "   'content': 'Adjusting the right PD. Is that better?'}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "### Input:\n",
      "Assistant: OK, so we'll start without any prescription using both eyes. It's going to be a little blurry, but try to read the smallest line that you can without squinting.\n",
      "Patient__: kdnro\n",
      "Assistant: OK, perfect. Now I'll cover the left eye for you, remember not to squint. What's the smallest row you can read?\n",
      "Patient__: Everything's blurry.\n",
      "Assistant: I see. Can you read the top line at all?\n",
      "Patient__: Yeah, RKDHC, but it's all blurred.\n",
      "\n",
      "### Response:\n",
      "\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: OK, blurring is to be expected because there's no prescription. I'll cover your right eye. Using your left eye, what's the smallest row you can read?\n",
      "\n",
      "MIXTRAL TEMPLATE\n",
      "<s>[INST]You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "Assistant: OK, so we'll start without any prescription using both eyes. It's going to be a little blurry, but try to read the smallest line that you can without squinting.\n",
      "Patient__: kdnro\n",
      "Assistant: OK, perfect. Now I'll cover the left eye for you, remember not to squint. What's the smallest row you can read?\n",
      "Patient__: Everything's blurry.\n",
      "Assistant: I see. Can you read the top line at all?\n",
      "Patient__: Yeah, RKDHC, but it's all blurred.\n",
      "[/INST]Assistant: OK, blurring is to be expected because there's no prescription. I'll cover your right eye. Using your left eye, what's the smallest row you can read?\n",
      "</s>\n",
      "Without response:\n",
      "<s>[INST]You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "Assistant: OK, so we'll start without any prescription using both eyes. It's going to be a little blurry, but try to read the smallest line that you can without squinting.\n",
      "Patient__: kdnro\n",
      "Assistant: OK, perfect. Now I'll cover the left eye for you, remember not to squint. What's the smallest row you can read?\n",
      "Patient__: Everything's blurry.\n",
      "Assistant: I see. Can you read the top line at all?\n",
      "Patient__: Yeah, RKDHC, but it's all blurred.\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "def format_mixtral(data, model_input=True):\n",
    "    instruction = 'You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.'\n",
    "    dialogue_string = ''\n",
    "    for i in data['dialogue']:\n",
    "        dialogue_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "    response_string = ''\n",
    "    for i in data['response']:\n",
    "        response_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "    if model_input:\n",
    "        return f'''<s>[INST]{instruction}\\n{dialogue_string}[/INST]{response_string}</s>'''\n",
    "    else:\n",
    "        return f'''<s>[INST]{instruction}\\n{dialogue_string}[/INST]'''\n",
    "\n",
    "def format_inference(data, model_input=True):\n",
    "    instruction = 'You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.'\n",
    "    dialogue_string = ''\n",
    "    for i in data['dialogue']:\n",
    "        dialogue_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "    if model_input:\n",
    "        return f'''### Instruction:\\n{instruction}\\n### Input:\\n{dialogue_string}\\n### Response:\\n'''\n",
    "    else:\n",
    "        response_string = ''\n",
    "        for i in data['response']:\n",
    "            response_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "        return f'''### Response:\\n{response_string}'''\n",
    "idx = randrange(test.__len__())\n",
    "sample = test.data[idx]\n",
    "print(format_inference(sample))\n",
    "print(f\"Ground truth:\\n{format_inference(sample, model_input=False)}\")\n",
    "print('MIXTRAL TEMPLATE')\n",
    "print(format_mixtral(sample))\n",
    "print(f\"Without response:\\n{format_mixtral(sample, model_input=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixtral max len\n",
      "train: 1427\n",
      "val: 1427\n",
      "test:1427\n",
      "\n",
      "Normal max len\n",
      "train: 1338\n",
      "val: 1338\n",
      "test:1338\n"
     ]
    }
   ],
   "source": [
    "def get_max_len(dataset,format_func):\n",
    "    max_len = 0\n",
    "    for sample in train.data:\n",
    "        inpt = format_func(sample)\n",
    "        if len(inpt) > max_len:\n",
    "            max_len = len(inpt)\n",
    "    return max_len\n",
    "\n",
    "max_len_train = get_max_len(train, format_mixtral)\n",
    "max_len_val = get_max_len(val, format_mixtral)\n",
    "max_len_test = get_max_len(test, format_mixtral)\n",
    "print('Mixtral max len')\n",
    "print(f'train: {max_len_train}\\nval: {max_len_val}\\ntest:{max_len_test}')\n",
    "max_len_train = get_max_len(train, format_inference)\n",
    "max_len_val = get_max_len(val, format_inference)\n",
    "max_len_test = get_max_len(test, format_inference)\n",
    "print('\\nNormal max len')\n",
    "print(f'train: {max_len_train}\\nval: {max_len_val}\\ntest:{max_len_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len exceeded 24 17\n",
      "max len exceeded 27 16\n",
      "max len exceeded 34 16\n",
      "max len exceeded 39 17\n",
      "max len exceeded 62 15\n",
      "max len exceeded 68 17\n",
      "max len exceeded 73 17\n",
      "max len exceeded 101 16\n",
      "max len exceeded 139 17\n",
      "max len exceeded 149 16\n",
      "max len exceeded 159 15\n",
      "max len exceeded 170 17\n",
      "max len exceeded 173 16\n",
      "max len exceeded 181 16\n",
      "max len exceeded 184 16\n",
      "max len exceeded 191 16\n",
      "max len exceeded 203 17\n",
      "max len exceeded 205 17\n",
      "max len exceeded 233 15\n",
      "max len exceeded 248 17\n",
      "max len exceeded 254 17\n",
      "max len exceeded 257 17\n",
      "max len exceeded 268 16\n",
      "max len exceeded 281 17\n",
      "max len exceeded 284 15\n",
      "max len exceeded 289 17\n",
      "max len exceeded 298 15\n",
      "max len exceeded 305 16\n",
      "max len exceeded 308 17\n",
      "max len exceeded 322 17\n",
      "max len exceeded 328 15\n",
      "max len exceeded 336 17\n",
      "max len exceeded 358 17\n",
      "max len exceeded 366 17\n",
      "max len exceeded 369 15\n",
      "max len exceeded 380 15\n",
      "max len exceeded 384 16\n",
      "max len exceeded 395 16\n",
      "max len exceeded 401 15\n",
      "max len exceeded 404 17\n",
      "max len exceeded 409 17\n",
      "max len exceeded 416 17\n",
      "max len exceeded 442 15\n",
      "max len exceeded 459 17\n",
      "max len exceeded 476 16\n",
      "max len exceeded 17 17\n",
      "max len exceeded 23 16\n",
      "max len exceeded 36 17\n",
      "max len exceeded 67 15\n",
      "max len exceeded 7 15\n",
      "max len exceeded 14 17\n",
      "max len exceeded 17 15\n",
      "max len exceeded 26 17\n",
      "max len exceeded 29 17\n",
      "max len exceeded 32 17\n",
      "max len exceeded 39 17\n",
      "max len exceeded 42 16\n",
      "max len exceeded 52 17\n",
      "max len exceeded 54 17\n",
      "max len exceeded 58 17\n",
      "max len exceeded 62 17\n",
      "max len exceeded 65 15\n",
      "max len exceeded 95 17\n"
     ]
    }
   ],
   "source": [
    "for dset in [train, val, test]:\n",
    "    for idx in range(len(dset.data)):\n",
    "        length = len(dset.data[idx][\"dialogue\"])\n",
    "        if length > 14:\n",
    "            print(\"max len exceeded\", idx, len(dset.data[idx][\"dialogue\"]))\n",
    "        if length < 2:\n",
    "            print(\"min len not met\", idx, len(dset.data[idx][\"dialogue\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train unique responses: 201\n",
      "val unique responses: 34\n",
      "test unique responses: 48\n"
     ]
    }
   ],
   "source": [
    "unique_responses = dict(\n",
    "    train=[],\n",
    "    val=[],\n",
    "    test=[]\n",
    ")\n",
    "\n",
    "splits = ['train', 'val', 'test']\n",
    "datasets = [train, val, test]\n",
    "for i in range(3):\n",
    "    dset = datasets[i]\n",
    "    for d in dset.data:\n",
    "        response = d[\"response\"][0][\"content\"]\n",
    "        if response not in unique_responses[splits[i]]:\n",
    "            unique_responses[splits[i]].append(response)\n",
    "\n",
    "for k in unique_responses.keys():\n",
    "    print(k, f'unique responses: {len(unique_responses[k])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'Assistant', 'content': \"Okay, good. So you're all set for distance. What we'll check next is the near vision. [LOCALTECH] will place a card in front of you. Let me know when the card is in place.\"}, {'role': 'Patient__', 'content': \"The card's already there.\"}, {'role': 'Assistant', 'content': 'Great! On this card, read the smallest line you can.'}, {'role': 'Patient__', 'content': 'A P E O R P D Z.'}, {'role': 'Assistant', 'content': \"Perfect. So I'll send that to the doctor. They're going to be on screen with you next. It was a pleasure working with you!\"}, {'role': 'Patient__', 'content': 'Okay.'}]\n",
      "Assistant: Okay, good. So you're all set for distance. What we'll check next is the near vision. [LOCALTECH] will place a card in front of you. Let me know when the card is in place.\n",
      "Patient__: The card's already there.\n",
      "Assistant: Great! On this card, read the smallest line you can.\n",
      "Patient__: A P E O R P D Z.\n",
      "Assistant: Perfect. So I'll send that to the doctor. They're going to be on screen with you next. It was a pleasure working with you!\n",
      "Patient__: Okay.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "def unpack(dialogue):\n",
    "    new_string = ''\n",
    "    for i in dialogue:\n",
    "        new_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "    return new_string\n",
    "idx = randrange(train.__len__())\n",
    "sample = train.data[idx]\n",
    "print(sample['dialogue'])\n",
    "print(unpack(sample['dialogue']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before formatting: \n",
      "{'dialogue': [{'role': 'Patient__', 'content': 'Yeah.'}, {'role': 'Assistant', 'content': 'Okay, the green side it better. Is it better on the red side, green side, or are they about the same?'}, {'role': 'Patient__', 'content': 'the same'}, {'role': 'Assistant', 'content': 'Okay. Blink a few times. Read the smallest row you can with your right eye.'}, {'role': 'Patient__', 'content': 'H Z. C, maybe K O.'}, {'role': 'Assistant', 'content': \"OK, now I'll test your left eye and we'll start again. Is it better with one? With two? Or the same?\"}, {'role': 'Patient__', 'content': 'One'}, {'role': 'Assistant', 'content': \"Alright, and same thing with dots on the screen. There's one. And two. Or are they the same?\"}, {'role': 'Patient__', 'content': 'to'}, {'role': 'Assistant', 'content': 'Are they better with one? Or two?'}, {'role': 'Patient__', 'content': \"i can't tell a difference.\"}, {'role': 'Assistant', 'content': 'OK. Which color looks better? Red? green? Or are they about the same?'}, {'role': 'Patient__', 'content': \"and the green tie, they're both about the same, you know.\"}], 'response': [{'role': 'Assistant', 'content': \"Perfect. Blink a few times and when you're ready, read the smallest line you can with your left eye.\"}]}\n",
      "\n",
      "After formatting: \n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create the Assistant's response that best guides Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: Yeah.\n",
      "Assistant: Okay, the green side it better. Is it better on the red side, green side, or are they about the same?\n",
      "Patient__: the same\n",
      "Assistant: Okay. Blink a few times. Read the smallest row you can with your right eye.\n",
      "Patient__: H Z. C, maybe K O.\n",
      "Assistant: OK, now I'll test your left eye and we'll start again. Is it better with one? With two? Or the same?\n",
      "Patient__: One\n",
      "Assistant: Alright, and same thing with dots on the screen. There's one. And two. Or are they the same?\n",
      "Patient__: to\n",
      "Assistant: Are they better with one? Or two?\n",
      "Patient__: i can't tell a difference.\n",
      "Assistant: OK. Which color looks better? Red? green? Or are they about the same?\n",
      "Patient__: and the green tie, they're both about the same, you know.\n",
      "\n",
      "\n",
      "### Response:\n",
      "Assistant: Perfect. Blink a few times and when you're ready, read the smallest line you can with your left eye.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "extra = \"A vision exam has several phases. Phase 1 consists of a greeting and the Local Tech will help the Patient__ get comfortable. Phase 2 consists of a visual acuity test. Phase 3 consists of a subjective refraction, where Optician switches lenses and the patient evaluates the choices offered. Phase 4 consists of a close vision test where a card is placed in front of the patient for a final evaluation.\"\n",
    "def format_instruction(data):\n",
    "    dialogue_string = ''\n",
    "    for i in data['dialogue']:\n",
    "        dialogue_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "    response_string = '' \n",
    "    for i in data['response']:\n",
    "        response_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "    return f'''### Instruction:\n",
    "You are an Optician conducting a vision exam. Use the dialogue below to create the Assistant's response that best guides Patient__ through a vision exam.\n",
    "\n",
    "### Input:\n",
    "{dialogue_string}\n",
    "\n",
    "### Response:\n",
    "{response_string}\n",
    "'''\n",
    "idx = randrange(train.__len__())\n",
    "print(f'Before formatting: \\n{train.data[idx]}\\n')\n",
    "print(f'After formatting: \\n{format_instruction(train.data[idx])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/digitalopt/miniconda3/envs/gemma-train/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flash attention\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m data_nm\u001b[38;5;241m=\u001b[39m data_dir\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# model_name = \"llama2-7b-int4-Exam_v1-30_2100-early_stop\" #epochs_trainsize\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m model_name \u001b[38;5;241m=\u001b[39m model_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-int4-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-early_stop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# model_name = \"Mistral-7B-Instruct-v0.2-int4-Exam_v1-30_2100-early_stop\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m mistral_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_len' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "use_flash_attention = False\n",
    "# COMMENT IN TO USE FLASH ATTENTION\n",
    "# replace attention with flash attention\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    from llama_patch import replace_attn_with_flash_attn\n",
    "    print(\"Using flash attention\")\n",
    "    replace_attn_with_flash_attn()\n",
    "    use_flash_attention = True\n",
    "\n",
    "\n",
    "###################### Hugging Face model ids ##################\n",
    "# model_id = \"NousResearch/Llama-2-7b-hf\" # non-gated\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n",
    "# model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# cache_dir = '/data/chat-models/local/NousResearch/Llama-2-7b-chat-hf'\n",
    "    \n",
    "# model_id = \"NousResearch/Llama-2-13b-hf\"\n",
    "    \n",
    "# model_id = \"NousResearch/Llama-2-13b-chat-hf\"\n",
    "# cache_dir = '/data/chat-models/foundation/NousResearch/Llama-2-13b-chat-hf'\n",
    "# model_id = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "\n",
    "model_id = \"Mistral-7B-Instruct-v0.2-int4-Exam_v3-20_480-early-stop\"\n",
    "cache_dir = '/data/chat-models/foundation/mistralai/Mistral-7B-Instruct-v0.2'\n",
    "# model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "\n",
    "# model_id = \"openchat/openchat-3.5-1210\"\n",
    "# cache_dir = '/data/chat-models/foundation/openchat/openchat-3.5-1210'\n",
    "\n",
    "\n",
    "##################### Local Models #######################\n",
    "# model_id = \"/data/localModels/Llama-2-13b-chat-hf\"\n",
    "# model_id =  '/data/chat-models/llama2/Llama-2-13b-chat-hf/'\n",
    "\n",
    "################## model names ###########################\n",
    "epochs = 30\n",
    "data_nm= data_dir.split('/')[-2]\n",
    "# model_name = \"llama2-7b-int4-Exam_v1-30_2100-early_stop\" #epochs_trainsize\n",
    "model_name = model_id.split('/')[-1] + f\"-int4-{data_nm}-{epochs}_{train_len}-early_stop\"\n",
    "\n",
    "# model_name = \"Mistral-7B-Instruct-v0.2-int4-Exam_v1-30_2100-early_stop\"\n",
    "mistral_model = False\n",
    "if model_id.split('/')[0] == \"mistralai\":\n",
    "    mistral_model = True\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\",\n",
    "                                             use_flash_attention_2=True,\n",
    "                                             trust_remote_code=False,\n",
    "                                             resume_download=True)\n",
    "\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,\n",
    "                                          trust_remote_code=False,\n",
    "                                          resume_download=True\n",
    "                                          )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.save_pretrained('/data/chat_models/foundation/' + model_id)\n",
    "tokenizer.save_pretrained('/data/chat_models/foundation/' + model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if mistral_model:\n",
    "#     idx = randrange(train.__len__())\n",
    "#     print(f'Before formatting: \\n{train.data[idx]}\\n')\n",
    "#     encodeds = tokenizer.apply_chat_template(train.data[idx], return_tensors=\"pt\")\n",
    "#     decoded = tokenizer.batch_decode(encodeds)\n",
    "#     print(f'After formatting: \\n{format_instruction(decoded)}')\n",
    "for name, mod in model.named_modules():\n",
    "    print(name, mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "# specify the linear layers of the mistral-7b model per the PEFT paper\n",
    "target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "]\n",
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= target_modules if mistral_model else None\n",
    ")\n",
    "\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Early stopping patience (number of epochs without improvement)\n",
    "early_stopping_patience = 3\n",
    "\n",
    "# Early stopping threshold (minimum relative improvement to continue training)\n",
    "early_stopping_threshold = -0.001\n",
    "\n",
    "# Create the callback\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience, early_stopping_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=30\n",
    "use_flash_attention=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_name, \n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=6 if use_flash_attention else 4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=True, # disable tqdm since with packing values are incorrect\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/digitalopt/miniconda3/envs/llama-train/lib/python3.11/site-packages/trl/trainer/utils.py:246: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictonnary returned by each element of the dataset. Make sure you know what you are doing.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train.data,\n",
    "    eval_dataset=val.data,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction,\n",
    "    args=args,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "trainer.save_model(model_id.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/digitalopt/miniconda3/envs/llama-train/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1644, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'eval_loss': 0.9461890459060669, 'eval_runtime': 24.6886, 'eval_samples_per_second': 12.151, 'eval_steps_per_second': 1.539, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/digitalopt/miniconda3/envs/llama-train/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9117, 'learning_rate': 0.0002, 'epoch': 1.04}\n",
      "{'eval_loss': 0.7030491232872009, 'eval_runtime': 24.1935, 'eval_samples_per_second': 12.4, 'eval_steps_per_second': 1.571, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/digitalopt/miniconda3/envs/llama-train/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7592, 'learning_rate': 0.0002, 'epoch': 2.02}\n",
      "{'loss': 0.6194, 'learning_rate': 0.0002, 'epoch': 2.07}\n",
      "{'eval_loss': 0.5447949767112732, 'eval_runtime': 24.3763, 'eval_samples_per_second': 12.307, 'eval_steps_per_second': 1.559, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/digitalopt/miniconda3/envs/llama-train/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5152, 'learning_rate': 0.0002, 'epoch': 3.05}\n",
      "{'eval_loss': 0.46092841029167175, 'eval_runtime': 24.3663, 'eval_samples_per_second': 12.312, 'eval_steps_per_second': 1.56, 'epoch': 3.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/digitalopt/miniconda3/envs/llama-train/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.45, 'learning_rate': 0.0002, 'epoch': 4.03}\n",
      "{'eval_loss': 0.42840448021888733, 'eval_runtime': 24.3885, 'eval_samples_per_second': 12.301, 'eval_steps_per_second': 1.558, 'epoch': 4.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/digitalopt/miniconda3/envs/llama-train/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3998, 'learning_rate': 0.0002, 'epoch': 5.01}\n",
      "{'loss': 0.3492, 'learning_rate': 0.0002, 'epoch': 5.07}\n",
      "{'eval_loss': 0.421701580286026, 'eval_runtime': 24.4018, 'eval_samples_per_second': 12.294, 'eval_steps_per_second': 1.557, 'epoch': 5.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/digitalopt/miniconda3/envs/llama-train/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3166, 'learning_rate': 0.0002, 'epoch': 6.05}\n",
      "{'eval_loss': 0.425231397151947, 'eval_runtime': 24.4608, 'eval_samples_per_second': 12.265, 'eval_steps_per_second': 1.554, 'epoch': 6.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/digitalopt/miniconda3/envs/llama-train/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2901, 'learning_rate': 0.0002, 'epoch': 7.03}\n",
      "{'eval_loss': 0.433732271194458, 'eval_runtime': 24.2426, 'eval_samples_per_second': 12.375, 'eval_steps_per_second': 1.567, 'epoch': 7.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/digitalopt/miniconda3/envs/llama-train/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2516, 'learning_rate': 0.0002, 'epoch': 8.01}\n",
      "{'loss': 0.2234, 'learning_rate': 0.0002, 'epoch': 8.07}\n",
      "{'eval_loss': 0.4603998064994812, 'eval_runtime': 23.0981, 'eval_samples_per_second': 12.988, 'eval_steps_per_second': 1.645, 'epoch': 8.07}\n",
      "{'train_runtime': 5500.9742, 'train_samples_per_second': 11.453, 'train_steps_per_second': 0.954, 'train_loss': 0.5191331606758528, 'epoch': 8.07}\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "\n",
    "# save model\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "trainer.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [00:00<00:00,  8.33it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.25s/it]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "tokenizer_config.json: 100%|██████████| 2.16k/2.16k [00:00<00:00, 24.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = ''\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_name, \n",
    ")\n",
    "###################### DEV MODEL\n",
    "# args.output_dir =\"/home/digitalopt/proj/chatbots/chatbot-v1/Llama-2-13b-chat-hf-int4-Exam_v2-30_420-early_stop\"\n",
    "# cache_dir = '/data/chat-models/foundation/NousResearch/Llama-2-13b-chat-hf'\n",
    "######################\n",
    "# args.output_dir = '/data/chat-models/local/' + model_name + '/'\n",
    "# args.output_dir = 'Llama-2-7b-chat-hf-int4-Exam_v2-30_525-early_stop'\n",
    "# args.output_dir = \"Llama-2-13b-chat-hf-int4-Exam_v2-30_525-early_stop\"\n",
    "\n",
    "# cache_dir = '/data/chat-models/foundation/NousResearch/Llama-2-13b-chat-hf'\n",
    "# args.output_dir = 'Llama-2-13b-chat-hf-int4-Exam_v3-10_320-early_stop'\n",
    "\n",
    "# cache_dir = '/data/chat-models/foundation/openchat/openchat-3.5-1210'\n",
    "# args.output_dir = 'openchat-3.5-1210-int4-Exam_v3-15_480-early_stop'\n",
    "# args.output_dir ='openchat-3.5-1210-int4-Exam_v2-15_420-early_stop' # <--- could be really good\n",
    "# args.output_dir = 'openchat-3.5-1210-int4-Exam_v3-3_480-early_stop'# <--- maybe pretty good?\n",
    "\n",
    "# cache_dir = '/data/chat-models/foundation/mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
    "# args.output_dir = 'Mixtral-8x7B-Instruct-v0.1-int4-Exam_v3-10_320-early_stop'\n",
    "\n",
    "# cache_dir = '/data/chat-models/foundation/openchat/openchat-3.5-0106'\n",
    "# args.output_dir = 'openchat-3.5-0106-int4-Exam_v3-15_480-early-stop'\n",
    "\n",
    "# cache_dir = '/data/chat-models/foundation/microsoft/phi-2'\n",
    "# args.output_dir = 'phi-2-int4-phoropter_v3-20_480-early-stop' # 'microsoft/phi-2'\n",
    "\n",
    "# cache_dir = '/data/chat-models/foundation/Intel/neural-chat-7b-v3-3'\n",
    "# args.output_dir = 'Intel/neural-chat-7b-v3-3'\n",
    "\n",
    "args.output_dir = \"google/gemma-7b-it\"\n",
    "cache_dir = '/data/chat-models/foundation/google/gemma-7b-it'\n",
    "\n",
    "phoropterModel = False\n",
    "foundation_model_nm = cache_dir.split('/')[4]\n",
    "# if args.output_dir[:3] != foundation_model_nm[:3]:\n",
    "#     raise Exception(f\"model in output_dir doesn't match model in cache dir\\noutput_dir:{args.output_dir}\\ncache_dir:{foundation_model_nm}\")\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     args.output_dir,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     load_in_4bit=True,\n",
    "#     # attn_implementation=\"flash_attention_2\",# use_flash_attention_2=True,\n",
    "#     temperature=0,\n",
    "#     cache_dir = cache_dir,\n",
    "#     trust_remote_code=True\n",
    "#     # device_map=\"auto\",\n",
    "# )\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(args.output_dir,\n",
    "                                            quantization_config=bnb_config,\n",
    "                                            # device_map='cuda',\n",
    "                                            device_map=\"auto\",\n",
    "                                            attn_implementation=\"flash_attention_2\",#use_flash_attention_2=True,\n",
    "                                            temperature=.9,\n",
    "                                            do_sample=True,\n",
    "                                            cache_dir= cache_dir,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            trust_remote_code=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir, cache_dir=cache_dir, padding_side='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 files found. Sampling 2 times per file.\n",
      "sampling file: /data/datasets/Exam_v3/test/000030.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000033.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000036.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000039.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000041.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000042.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000043.txt\n",
      "sampling file: /data/datasets/Exam_v3/test/000044.txt\n",
      "Expected length of data: 18\n",
      "Actual length: 16\n",
      "5 files found. Sampling 3 times per file.\n",
      "sampling file: /data/datasets/Exam_v3/val/000034.txt\n",
      "sampling file: /data/datasets/Exam_v3/val/000035.txt\n",
      "sampling file: /data/datasets/Exam_v3/val/000037.txt\n",
      "sampling file: /data/datasets/Exam_v3/val/000038.txt\n",
      "sampling file: /data/datasets/Exam_v3/val/000040.txt\n",
      "Expected length of data: 15\n",
      "Actual length: 15\n",
      "16\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "from vision_dataset import VisionDatasetCreator, VisionDataset\n",
    "import os \n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF']='max_split_size_mb:50'\n",
    "# avg percentages of exam phase lengths\n",
    "gs_len=0.25\n",
    "sp_len=0.25\n",
    "ac_len=0.25\n",
    "cv_len=0.25\n",
    "# percentage of samples taken from each exam phase\n",
    "gs_ratio=0.25\n",
    "sp_ratio=0.25\n",
    "ac_ratio=0.25\n",
    "cv_ratio=0.25\n",
    "if phoropterModel:\n",
    "    # minumum dialogue lengths\n",
    "    gs_min = 2\n",
    "    sp_min = 4\n",
    "    ac_min = 4\n",
    "    cv_min = 4\n",
    "    # maximum dialogue lengths\n",
    "    gs_max = 6\n",
    "    sp_max = 6\n",
    "    ac_max = 6\n",
    "    cv_max = 6\n",
    "else:\n",
    "    # minumum dialogue lengths\n",
    "    gs_min = 2\n",
    "    sp_min = 4\n",
    "    ac_min = 8\n",
    "    cv_min = 6\n",
    "    # maximum dialogue lengths\n",
    "    gs_max = 5\n",
    "    sp_max = 14\n",
    "    ac_max = 18\n",
    "    cv_max = 10\n",
    "\n",
    "\n",
    "sampling_strategy = dict(\n",
    "    gs=[gs_len, gs_ratio, gs_min, gs_max],\n",
    "    sp=[sp_len, sp_ratio, sp_min, sp_max],\n",
    "    ac=[ac_len, ac_ratio, ac_min, ac_max],\n",
    "    cv=[cv_len, cv_ratio, cv_min, cv_max]\n",
    ")\n",
    "\n",
    "if phoropterModel:\n",
    "    data_dir = '/data/datasets/phoropter_v3/'\n",
    "    dataset_creator = VisionDatasetCreator(sampling_strategy, seed=42, assistant=False)\n",
    "else:\n",
    "    data_dir = '/data/datasets/Exam_v3/'\n",
    "    # random seed for testing\n",
    "    dataset_creator = VisionDatasetCreator(sampling_strategy)\n",
    "\n",
    "\n",
    "\n",
    "num_samples = 3\n",
    "# just use test set\n",
    "size = (6*num_samples)\n",
    "dataset_creator.load(data_dir, 'test', size)\n",
    "size = (5*num_samples)\n",
    "dataset_creator.load(data_dir, 'val', size)\n",
    "\n",
    "print(len(dataset_creator.dataset['test']))\n",
    "test = VisionDataset(dataset_creator.dataset[\"test\"])\n",
    "print(len(dataset_creator.dataset['val']))\n",
    "val = VisionDataset(dataset_creator.dataset[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sequence length: 625\n",
      "length of test set: 31\n"
     ]
    }
   ],
   "source": [
    "big_test = test.data + val.data\n",
    "\n",
    "max_len = 0\n",
    "for i in test.data:\n",
    "    data = i['dialogue']\n",
    "    lenD = 0\n",
    "    for d in data:\n",
    "        lenD += len(d[\"content\"])\n",
    "    if lenD > max_len:\n",
    "        max_len = lenD\n",
    "print(f'max sequence length: {max_len}\\nlength of test set: {len(big_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "Patient__: Yes.\n",
      "Assistant: Okay, open your eyes. I put in your current prescription. Just like before, we'll start with both eyes. When you're ready, read the smallest row you can, ok?\n",
      "Patient__: OZRSN.\n",
      "Assistant: Good job. Read the smallest line you can with the right eye.\n",
      "Patient__: O R K S E.\n",
      "Assistant: Good. What's the smallest row you can read with your left eye?\n",
      "Patient__: NCKHD.\n",
      "Assistant: Thank you. Now close your eyes again for me. Are they closed?\n",
      "Patient__: Yes.\n",
      "Assistant: Now open your eyes. I'm testing your right eye first. I'll show you two choices and you will pick the one that looks most clear. Which is better, choice one? Choice two? Or are they similar?\n",
      "Patient__: Can you go back?\n",
      "[/INST]Assistant: Of course. This is number one. This is number two. Or are they about the same?\n",
      "</s>\n",
      "Ground truth:\n",
      "<s>[INST]You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "Patient__: Yes.\n",
      "Assistant: Okay, open your eyes. I put in your current prescription. Just like before, we'll start with both eyes. When you're ready, read the smallest row you can, ok?\n",
      "Patient__: OZRSN.\n",
      "Assistant: Good job. Read the smallest line you can with the right eye.\n",
      "Patient__: O R K S E.\n",
      "Assistant: Good. What's the smallest row you can read with your left eye?\n",
      "Patient__: NCKHD.\n",
      "Assistant: Thank you. Now close your eyes again for me. Are they closed?\n",
      "Patient__: Yes.\n",
      "Assistant: Now open your eyes. I'm testing your right eye first. I'll show you two choices and you will pick the one that looks most clear. Which is better, choice one? Choice two? Or are they similar?\n",
      "Patient__: Can you go back?\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "def format_mixtral(data, model_input=True):\n",
    "    instruction = 'You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.'\n",
    "    dialogue_string = ''\n",
    "    for i in data['dialogue']:\n",
    "        dialogue_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "    response_string = ''\n",
    "    for i in data['response']:\n",
    "        response_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "    if model_input:\n",
    "        return f'''<s>[INST]{instruction}\\n{dialogue_string}[/INST]{response_string}</s>'''\n",
    "    else:\n",
    "        return f'''<s>[INST]{instruction}\\n{dialogue_string}[/INST]'''\n",
    "\n",
    "def format_inference(data, model_input=True):\n",
    "    dialogue_string = ''\n",
    "    for i in data['dialogue']:\n",
    "        dialogue_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "    if model_input:\n",
    "        return f'''### Instruction:\n",
    "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
    "\n",
    "### Input:\n",
    "{dialogue_string}\n",
    "\n",
    "### Response:\n",
    "'''\n",
    "    else:\n",
    "        response_string = ''\n",
    "        for i in data['response']:\n",
    "            response_string += f'''{i['role']}: {i['content']}\\n'''\n",
    "        return f'''### Response:\\n{response_string}'''\n",
    "idx = randrange(test.__len__())\n",
    "sample = test.data[idx]\n",
    "# print(format_inference(sample))\n",
    "# print(f\"Ground truth:\\n{format_inference(sample, model_input=False)}\")\n",
    "print(format_mixtral(sample))\n",
    "print(f\"Ground truth:\\n{format_mixtral(sample, model_input=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "968"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for sample in test.data:\n",
    "    inpt = format_mixtral(sample)\n",
    "    if len(inpt) > max_len:\n",
    "        max_len = len(inpt)\n",
    "max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/digitalopt/miniconda3/envs/gemma-train/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/digitalopt/miniconda3/envs/gemma-train/lib/python3.11/site-packages/transformers/generation/utils.py:1342: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 4.046 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: Sure. Adjusting the right PD by one step. Is that good?\n",
      "Patient__: Looks good and I think we're ready.\n",
      "Assistant: Great! We'll start without any prescription using both eyes. The letters might look a little blurry. Without squinting, read the smallest line you can.\n",
      "Patient__: It's blurry, but ZHC.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here is the Assistant's response:\n",
      "\n",
      "\"Sure, I understand you're ready. Let's begin the exam without any prescription. I'll show you the smallest line you can read. Just focus on the letters and let me know if they're clear or if they're blurry. If they're blurry, I'll need you to say which letters are blurry so I can make adjustments.\"\n",
      "\n",
      "\n",
      "**Additional notes:**\n",
      "\n",
      "- The Assistant should be friendly and\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Great! I'm covering your left eye, now. What's the smallest row you can read without squinting with your right eye?\n",
      "\n",
      "Similarity Score: [[0.338]]_______\n",
      "____________________\n",
      "elapsed time: 3.678 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: Sure. Adjusting both PDs in by one. Is that better?\n",
      "LocalTech: Okay.\n",
      "Assistant: Are we ready to start the exam?\n",
      "LocalTech: Sure. Go ahead.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, [Patient Name], I'm here to guide you through the exam. Please let me know if you're comfortable with me proceeding.\n",
      "\n",
      "**Note:** The patient's name is not provided in the text, therefore I have inserted \"Patient Name\" as a placeholder.\n",
      "\n",
      "**Additional Information:**\n",
      "- The patient has already completed the PD adjustment.\n",
      "- The assistant is ready to begin the exam.\n",
      "\n",
      "**Assistant Response:**\n",
      "\n",
      "\"Sure, [Patient Name], I'\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: OK, so we'll start without any prescription using both eyes. It's going to be a little blurry, but try to read the smallest line that you can without squinting.\n",
      "\n",
      "Similarity Score: [[0.189]]_______\n",
      "____________________\n",
      "elapsed time: 3.676 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: Good job. Read the smallest line you can with the right eye.\n",
      "Patient__: husde\n",
      "Assistant: Good. What's the smallest row you can read with your left eye?\n",
      "Patient__: boaem\n",
      "Assistant: Thank you. Now close your eyes again for me. Are they closed?\n",
      "Patient__: Yeah\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "**Assistant:** \"Sure, I understand. Please read the smallest line you can see with your right eye. If you need help, just let me know.\"\n",
      "\n",
      "**Patient:** \"husde\"\n",
      "\n",
      "**Assistant:** \"Okay, I've got it. Now, read the smallest row you can see with your left eye. If you need help, just say the word 'help'.\"\n",
      "\n",
      "**Patient:** \"boaem\"\n",
      "\n",
      "**Assistant:** \"Got it. Thank you for your time\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Now open your eyes. I'm testing your right eye first. I'll show you two choices and you will pick the one that looks most clear. Which is better, choice one? Choice two? Or are they similar?\n",
      "\n",
      "Similarity Score: [[0.331]]_______\n",
      "____________________\n",
      "elapsed time: 3.687 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: Great! We'll start without any prescription using both eyes. The letters might look a little blurry. Without squinting, read the smallest line you can.\n",
      "Patient__: Uh, BCKMG.\n",
      "Assistant: OK, perfect. Now I'll cover the left eye for you, remember not to squint. What's the smallest row you can read?\n",
      "Patient__: that are all very blurry.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here's what I'll do. I'll explain the process of the exam and guide you through each step. First, I'll start without any prescription using both eyes. The letters might look a little blurry, but don't squint. Read the smallest line you can.\n",
      "\n",
      "**Please let me know if you have any trouble reading the letters.**\n",
      "\n",
      "**Please note:** I'm covering the left eye for you now, so you shouldn't squint\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: No worries! Let me enlarge the text for you. Read the smallest line you can now.\n",
      "\n",
      "Similarity Score: [[0.271]]_______\n",
      "____________________\n",
      "elapsed time: 3.668 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: No problem. I'll make it bigger, can you make out what letter this is?\n",
      "Patient__: The outline looks like a B.\n",
      "Assistant: OK, perfect. Now I'll cover the left eye for you, remember not to squint. What's the smallest letter you can read?\n",
      "Patient__: S\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here is the Assistant's response:\n",
      "\n",
      "**Assistant:** \"No problem. I'll make it bigger, can you make out what letter this is? You said the outline looks like a B. Okay, perfect. Now I'm going to cover your left eye for you, so you'll need to keep your eyes open and not squint. Can you tell me the smallest letter you can read?\"\n",
      "\n",
      "**Assistant:** \"I'm sorry, but I couldn'\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: I'll cover your right eye. Using your left eye, what's the smallest letter you can read?\n",
      "\n",
      "Similarity Score: [[0.602]]_______\n",
      "____________________\n",
      "elapsed time: 3.699 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "LocalTech: Good, thanks. This is [PATIENT].\n",
      "Assistant: Perfect! I'm [ASSISTANT]. I'll guide you though the exam before you see the doctor today. [LOCALTECH] will get you set up. [LOCALTECH], just say \"[ASSISTANT], start the exam\", and I'll know you're ready to start. If the PDs need adjusting, I can do that, too.\n",
      "Patient__: Okay.\n",
      "LocalTech: Thank you. It won't be very long.  Okay. Ready to start.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here's the Assistant's response:\n",
      "\n",
      "**Assistant:** \"Hi [PATIENT], I'm [ASSISTANT]. I'm here to guide you through your vision exam. Just say \"[ASSISTANT], start the exam\", and I'll know you're ready to begin. If your PDs need adjusting, I can do that as well. So, are you ready to start?\"\n",
      "\n",
      "**Note:** The Assistant's response should be clear, concise, and friendly\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: OK, so we'll start without any prescription using both eyes. It's going to be a little blurry, but try to read the smallest line that you can without squinting.\n",
      "\n",
      "Similarity Score: [[0.408]]_______\n",
      "____________________\n",
      "elapsed time: 3.692 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: Yes.\n",
      "Assistant: Okay, open your eyes. I put in your current prescription. Just like before, we'll start with both eyes. When you're ready, read the smallest row you can, ok?\n",
      "Patient__: OZRSN.\n",
      "Assistant: Good job. Read the smallest line you can with the right eye.\n",
      "Patient__: O R K S E.\n",
      "Assistant: Good. What's the smallest row you can read with your left eye?\n",
      "Patient__: NCKHD.\n",
      "Assistant: Thank you. Now close your eyes again for me. Are they closed?\n",
      "Patient__: Yes.\n",
      "Assistant: Now open your eyes. I'm testing your right eye first. I'll show you two choices and you will pick the one that looks most clear. Which is better, choice one? Choice two? Or are they similar?\n",
      "Patient__: Can you go back?\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here is the Assistant's response:\n",
      "\n",
      "**Assistant:** \"Okay, open your eyes. I put in your current prescription. Just like before, we'll start with both eyes. When you're ready, read the smallest row you can, ok? Please read the smallest line you can with your right eye. Read the line below the line you just read. Now, read the smallest line you can with your left eye. Thank you. Now close your eyes again for me\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Of course. This is number one. This is number two. Or are they about the same?\n",
      "\n",
      "Similarity Score: [[0.107]]_______\n",
      "____________________\n",
      "elapsed time: 3.690 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: uh huh\n",
      "Assistant: You can open your eyes. I put in your current prescription. Let's use both eyes again. Without squinting, read the smallest row you can, ok?\n",
      "Patient__: ZKCSV\n",
      "Assistant: Good job. Read the smallest line you can with the right eye.\n",
      "Patient__: um, N D or N O V K D.\n",
      "Assistant: Good. What's the smallest row you can read with your left eye?\n",
      "Patient__: O Z R F N\n",
      "Assistant: Thank you. Now close your eyes again for me. Are they closed?\n",
      "Patient__: yeah\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here is the Assistant's response:\n",
      "\n",
      "**Assistant:** \"Okay, I've put in your current prescription. Can you open your eyes and read the smallest row you can see, without squinting? Please read the row letter by letter, and let me know if you need me to repeat the instructions.\"\n",
      "\n",
      "**Note:** This response is designed to guide the Patient through the various steps of the vision exam, ensuring they understand each step and are able to complete the exam properly.\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Now open your eyes. I'm testing your right eye first. I'll show you two choices and you will pick the one that looks most clear. Which is better, choice one? Choice two? Or are they similar?\n",
      "\n",
      "Similarity Score: [[0.332]]_______\n",
      "____________________\n",
      "elapsed time: 3.675 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: Um, O N B S R.\n",
      "Assistant: Good. What's the smallest row you can read with your left eye?\n",
      "Patient__: KDRDG\n",
      "Assistant: Thank you. Now close your eyes. Are they closed?\n",
      "Patient__: closed\n",
      "Assistant: Now open your eyes. I blocked your left eye. I'm going to show you two choices. You will pick the one that looks more clear.  We will start with the right eye. Here is choice one. Here is choice two. Which is better? Or are they similar?\n",
      "Patient__: One.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "**Assistant:** \"Sure, Mr. O N B S R, I understand you're saying you can read the smallest row, KDRDG, with your left eye. I'm going to guide you through the rest of the exam. Please close your eyes and let me know if your eyes are closed. Once I've blocked your left eye, I'll show you two choices and you'll pick the one that looks more clear. Okay?\"\n",
      "\n",
      "**Additional notes:**\n",
      "\n",
      "\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: We're going to do the same thing now with dots. Are the dots better with one? With two? Or are they the same?\n",
      "\n",
      "Similarity Score: [[0.165]]_______\n",
      "____________________\n",
      "elapsed time: 3.720 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: OK, so we'll start without any prescription using both eyes. It's going to be a little blurry, but try to read the smallest line that you can without squinting.\n",
      "Patient__: S is, excuse me, SZR.\n",
      "Assistant: OK, perfect. Now I'll cover the left eye for you, remember not to squint. What's the smallest row you can read?\n",
      "Patient__: Um, S Z R.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "**Assistant:** Sure, I understand that the first step is to see how well you can see without any prescription. It's a little blurry, but I'm hoping you can read the smallest line you can without squinting. Can you tell me what you see?\n",
      "\n",
      "**Additional Notes:**\n",
      "\n",
      "* The Assistant should be friendly and professional.\n",
      "* The Assistant should explain each step of the process clearly and concisely.\n",
      "* The Assistant should be patient and understanding.\n",
      "* The Assistant\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Let's cover the right eye. What's the smallest row you can read with your left eye?\n",
      "\n",
      "Similarity Score: [[0.433]]_______\n",
      "____________________\n",
      "elapsed time: 3.647 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: Okay, go ahead and read it.\n",
      "Patient__: P C K N O\n",
      "Assistant: Great! I'm covering your left eye, now. What's the smallest row you can read without squinting with your right eye?\n",
      "Patient__: O H V C K\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here's my assistant response:\n",
      "\n",
      "\"Alright, [Patient's name], I'm going to start by covering your left eye. Can you tell me the smallest row you can read on the chart with your right eye without squinting? Please say the letters clearly.\"\n",
      "\n",
      "**Additional notes:**\n",
      "\n",
      "- The Assistant should be friendly and professional.\n",
      "- The Assistant should be clear and concise.\n",
      "- The Assistant should be patient and understanding.\n",
      "\n",
      "**Assistant response:**\n",
      "\n",
      "\"\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Let's cover the right eye. What's the smallest row you can read with your left eye?\n",
      "\n",
      "Similarity Score: [[0.576]]_______\n",
      "____________________\n",
      "elapsed time: 3.564 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: Perfect! I'm [ASSISTANT]. I'll guide you though the exam before you see the doctor today. [LOCALTECH] will get you set up. [LOCALTECH], just say \"[ASSISTANT], start the exam\", and I'll know you're ready to start. If the PDs need adjusting, I can do that, too.\n",
      "LocalTech: Let me get those PDs on if you wanted to double check those. I'll double check though.\n",
      "Assistant: [LOCALTECH], let me know if you need the PDs adjusted, or just say \"Start the exam\", and I'll jump right in.\n",
      "LocalTech: You're all set. Start the exam.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "**Sure, here is the Assistant's response:**\n",
      "\n",
      "\"Hi, [Patient Name]. I'm here to guide you through your vision exam. Please let me know if you're ready. Just say \"[ASSISTANT], start the exam\", and I'll know you're ready to begin. If you need me to adjust your PDs, I can do that too. Just let me know. I'm ready to start the exam whenever you are.\"\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Great! We'll start without any prescription using both eyes. The letters might look a little blurry. Without squinting, read the smallest line you can.\n",
      "\n",
      "Similarity Score: [[0.401]]_______\n",
      "____________________\n",
      "elapsed time: 3.664 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: n o k h d\n",
      "Assistant: Thank you. Now close your eyes again for me. Are they closed?\n",
      "Patient__: Okay\n",
      "Assistant: Now open your eyes. I'm testing your right eye first. I'll show you two choices and you will pick the one that looks most clear. Which is better, choice one? Choice two? Or are they similar?\n",
      "Patient__: to.\n",
      "Assistant: We're going to do the same thing now with dots. Are the dots better with one? With two? Or are they the same?\n",
      "Patient__: Go back to one.\n",
      "Assistant: Sure! Here's one. There's two. Or are they too similar?\n",
      "Patient__: two\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "**Assistant:** \"Alright, [Patient name], I've completed the initial part of the exam. I'm going to test your vision with a few different charts. First, I'm going to show you a few lines and you'll tell me which one looks best. Which line is the clearest, choice one or choice two?\n",
      "\n",
      "**Assistant:** \"I'm moving on to the next part of the exam, which involves dots. Can you tell me which group of dots\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Are the dots better with one? Two? Or are they about the same?\n",
      "\n",
      "Similarity Score: [[0.587]]_______\n",
      "____________________\n",
      "elapsed time: 3.641 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: Z\n",
      "Assistant: OK, perfect. Now I'll cover the left eye for you, remember not to squint. What's the smallest row you can read?\n",
      "Patient__: E\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "**Assistant:** Okay, Z, I've covered your left eye. Please read the smallest row of letters you can see on the chart.\n",
      "\n",
      "**Patient:** E\n",
      "\n",
      "**Assistant:** Okay, I've noted that you can read the E row. Now I'll move on to your right eye. Please keep your eyes open and I'll show you the next part of the exam.\n",
      "```\n",
      "\n",
      "## Assistant Response:\n",
      "\n",
      "\"Alright, Z, I've covered your left\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Let's cover the right eye. What's the smallest row you can read with your left eye?\n",
      "\n",
      "Similarity Score: [[0.492]]_______\n",
      "____________________\n",
      "elapsed time: 3.653 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: kdnro\n",
      "Assistant: OK, perfect. Now I'll cover the left eye for you, remember not to squint. What's the smallest row you can read?\n",
      "Patient__: Everything's blurry.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Assistant: No problem, I'll try to help you. Can you tell me the first letter you can see on the top line of the chart?\n",
      "\n",
      "**Note:** The Assistant should guide the Patient_ through the exam steps and explain each step clearly.\n",
      "\n",
      "**Assistant:** Sure, I'm here to guide you through the exam. First, I'm going to cover your left eye and see what you can see. Is there anything you can read on the top line of the chart\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: I see. Can you read the top line at all?\n",
      "\n",
      "Similarity Score: [[0.327]]_______\n",
      "____________________\n",
      "elapsed time: 3.655 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: yeah\n",
      "Assistant: Now open your eyes. I'm testing your right eye first. I'll show you two choices and you will pick the one that looks most clear. Which is better, choice one? Choice two? Or are they similar?\n",
      "Patient__: Number two.\n",
      "Assistant: We're going to do the same thing now with dots. Are the dots better with one? With two? Or are they the same?\n",
      "Patient__: number two\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here is an Assistant response that guides the Patient__ through the vision exam:\n",
      "\n",
      "**Assistant:** \"Alright, [Patient name], I'm going to begin your vision exam. I'm going to start with your right eye. I'm going to show you two choices and you're going to pick the one that looks most clear. Which is better, choice one or choice two? Or are they similar?\"\n",
      "\n",
      "**Patient:** \"Number two.\"\n",
      "\n",
      "**Assistant:** \"\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Are the dots better with one? Two? Or are they about the same?\n",
      "\n",
      "Similarity Score: [[0.35]]_______\n",
      "____________________\n",
      "elapsed time: 3.663 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: Good job. Read the smallest line you can with the right eye.\n",
      "Patient__: Yeah, a little bit blurry, but not too bad.\n",
      "Assistant: Can you read the smallest row you can for me?\n",
      "Patient__: a e b o d\n",
      "Assistant: Good. What's the smallest row you can read with your left eye?\n",
      "Patient__: Uh, same, it's a little bit blurry. u h m s e\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "**Assistant:** \"I understand that you're saying the vision in both eyes is a little blurry, but I'm glad you're able to read the smallest line. Can you read the smallest row you can see on the chart with your right eye?\"\n",
      "\n",
      "**Patient:** \"Yeah, a little bit blurry, but not too bad.\"\n",
      "\n",
      "**Assistant:** \"Okay, I understand. Can you read the smallest row you can see on the chart with your left eye?\"\n",
      "\n",
      "**Patient\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Thank you. Now close your eyes again for me. Are they closed?\n",
      "\n",
      "Similarity Score: [[0.246]]_______\n",
      "____________________\n",
      "elapsed time: 3.741 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: Last comparison! Blink a few times. Is it better with number one? Or with number two?\n",
      "Patient__: two\n",
      "Assistant: Perfect. Choice one is where we started. Choice two is the new prescription, that's what we found today. Does everything look clear?\n",
      "Patient__: Two is the new glasses?\n",
      "Assistant: It sure is! So you're all set for distance. What we'll check next is your near vision. [LOCALTECH] will place a card in front of you. Let me know when the card is in place.\n",
      "LocalTech: I put a card down.\n",
      "Assistant: Great! On this card, read the smallest line you can.\n",
      "Patient__: A P E O R F D C.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here is an Assistant response that guides the Patient__ through a vision exam based on the input:\n",
      "\n",
      "**Assistant:** \"Last comparison! Blink a few times. Is it better with number one? Or with number two?\"\n",
      "\n",
      "**Patient:** \"Two.\"\n",
      "\n",
      "**Assistant:** \"Perfect. Choice one is where we started. Choice two is the new prescription, that's what we found today. Does everything look clear?\"\n",
      "\n",
      "**Patient:** \"Two is the new glasses?\"\n",
      "\n",
      "**\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: You did well! You can relax while I send this information to the doctor. They're going to be on screen with you next. It was a pleasure working with you!\n",
      "\n",
      "Similarity Score: [[0.174]]_______\n",
      "____________________\n",
      "elapsed time: 3.702 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: I can maybe guess on the middle row.  Let me try blinking them.  S, R, Z, K, D.\n",
      "Assistant: Good. What's the smallest row you can read with your left eye?\n",
      "Patient__: No, I can almost get the bottom one.  O N R K G or Q, the middle N V D O K.\n",
      "Assistant: Thank you. Now close your eyes again for me. Are they closed?\n",
      "Patient__: yes\n",
      "Assistant: Now open your eyes. I'm testing your right eye first. I'll show you two choices and you will pick the one that looks most clear. Which is better, choice one? Choice two? Or are they similar?\n",
      "Patient__: two is clearer\n",
      "Assistant: We're going to do the same thing now with dots. Are the dots better with one? With two? Or are they the same?\n",
      "Patient__: Those look more clear here at 1.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "**Assistant:**\n",
      "\n",
      "\"Good. What's the smallest row you can read with your left eye? Please tell me the letters you can see.\"\n",
      "\n",
      "**Patient:** \"No, I can almost get the bottom one. O N R K G or Q, the middle N V D O K.\"\n",
      "\n",
      "**Assistant:** \"Thank you. Now close your eyes again for me. Are they closed?\"\n",
      "\n",
      "**Patient:** \"yes\"\n",
      "\n",
      "**Assistant:** \"Now open your eyes. I'\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Are they better with one? Or two?\n",
      "\n",
      "Similarity Score: [[0.114]]_______\n",
      "____________________\n",
      "elapsed time: 3.679 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: d e v c s\n",
      "Assistant: Great! What's the smallest row you can read with your left eye?\n",
      "Patient__: opzag\n",
      "Assistant: Thank you. Now close your eyes again for me. Are they closed?\n",
      "Patient__: Yeah\n",
      "Assistant: Now open your eyes. I'm testing your right eye first. I'll show you two choices and you will pick the one that looks most clear. Which is better, choice one? Choice two? Or are they similar?\n",
      "Patient__: one\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here's your Assistant response:\n",
      "\n",
      "**Assistant:** \"Hi, d e v c s. I'm here to conduct your vision exam. First, I'd like you to tell me the smallest row you can read with your left eye. Can you tell me the letters?\"\n",
      "\n",
      "**Patient:** \"opzag.\"\n",
      "\n",
      "**Assistant:** \"Thank you. Now I need you to close your eyes again for me. Are your eyes closed?\"\n",
      "\n",
      "**Patient:** \"Yeah.\"\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: We're going to do the same thing now with dots. Are the dots better with one? With two? Or are they the same?\n",
      "\n",
      "Similarity Score: [[0.128]]_______\n",
      "____________________\n",
      "elapsed time: 3.652 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: I'll cover your right eye. Using your left eye, what's the smallest row you can read?\n",
      "Patient__: HRDKG.\n",
      "Assistant: Good. Close your eyes for a moment while I make a change. Are they closed?\n",
      "Patient__: Yes\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Assistant: Okay, I've made the change. I'm going to show you the same row again, but this time with the left eye covered. Can you read it again?\n",
      "\n",
      "**Note:** The Assistant should explain the change made, and then ask the Patient to read the same row again, but this time with the left eye covered.\n",
      "\n",
      "\n",
      "**Desired Assistant Response:**\n",
      "\n",
      "**Assistant:** Okay, I've made the change. I'm going to show you the same row\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: You can open your eyes. I put in your current prescription. Let's use both eyes again. Without squinting, read the smallest row you can, ok?\n",
      "\n",
      "Similarity Score: [[0.417]]_______\n",
      "____________________\n",
      "elapsed time: 3.701 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: Z, X, A, R.\n",
      "Assistant: OK, now I'll test your left eye and we'll start again. Is it better with one? With two? Or are they the same?\n",
      "Patient__: One.\n",
      "Assistant: Great! Let's compare the dots again. Is it better with one? Two? Or are they the same?\n",
      "Patient__: One.\n",
      "Assistant: Is it better with one, with two, or are they similar?\n",
      "Patient__: Two.\n",
      "Assistant: Again, are the dots better with one, two, or are they similar?\n",
      "Patient__: To.\n",
      "Assistant: Are they better with one? Or two?\n",
      "Patient__: the same\n",
      "Assistant: Which color looks the most clear? The red side, green side, or are they about the same?\n",
      "Patient__: They look easy.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here is the Assistant's response:\n",
      "\n",
      "**Assistant:** \"Alright, Z, X, A, R, I'm going to test your left eye first. Can you tell me if the dots are better with one eye, with two eyes, or are they the same?\"\n",
      "\n",
      "**Patient:** \"One.\"\n",
      "\n",
      "**Assistant:** \"Okay, I'll try again. Is it better with one eye, with two eyes, or are they the same?\"\n",
      "\n",
      "**Patient:**\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Help me understand. Which side looks the most clear? The red, the green, or are they similar?\n",
      "\n",
      "Similarity Score: [[0.331]]_______\n",
      "____________________\n",
      "elapsed time: 3.679 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: Hi, how are you?\n",
      "Assistant: Hello! I'm doing great! Is [PATIENT] with us today?\n",
      "LocalTech: We're good.  I'm going to check his PDs real quick.  Sure, got those lights for you.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, [Patient's name], I'm here to help you with your vision exam. Would you like me to explain the process to you in more detail?\n",
      "\n",
      "**Assistant:**\n",
      "\n",
      "**Sure, [Patient's name], I'm here to help you with your vision exam. Would you like me to explain the process to you in more detail?**\n",
      "\n",
      "**Additional Information:**\n",
      "\n",
      "* The Assistant should be friendly and professional.\n",
      "* The Assistant should be able to answer the\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Perfect! I'm [ASSISTANT]. I'll guide you though the exam before you see the doctor today. [LOCALTECH] will get you set up. [LOCALTECH], just say \"[ASSISTANT], start the exam\", and I'll know you're ready to start. If the PDs need adjusting, I can do that, too.\n",
      "\n",
      "Similarity Score: [[0.568]]_______\n",
      "____________________\n",
      "elapsed time: 3.693 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: yeah, it is.\n",
      "Assistant: We're going to do the same thing now with dots. Are the dots better with one? With two? Or are they the same?\n",
      "Patient__: That one.\n",
      "Assistant: To be clear, you said number two is better?\n",
      "Patient__: Yes.\n",
      "Assistant: Are the dots better with one? Two? Or are they about the same?\n",
      "Patient__: two\n",
      "Assistant: Are they better with one? Or two?\n",
      "Patient__: two.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here is the Assistant's response:\n",
      "\n",
      "**Assistant:** \"Okay, I understand. You said the dots are better with two. Can you tell me if they're better with one or are they about the same?\"\n",
      "\n",
      "**Patient:** \"Two.\"\n",
      "\n",
      "**Assistant:** \"So, you said the dots are better with two. Are you sure?\"\n",
      "\n",
      "**Patient:** \"Yes.\"\n",
      "\n",
      "**Assistant:** \"Okay, I've noted that. We're finished with the\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Are the dots better with one? Two? Or are they about the same?\n",
      "\n",
      "Similarity Score: [[0.558]]_______\n",
      "____________________\n",
      "elapsed time: 3.730 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: Good. Close your eyes for a moment while I make a change. Are they closed?\n",
      "Patient__: yes they are\n",
      "Assistant: You can open your eyes. I put in your current prescription. Let's use both eyes again. Without squinting, read the smallest row you can, ok?\n",
      "Patient__: B U K L O.\n",
      "Assistant: Good job. Read the smallest line you can with the right eye.\n",
      "Patient__: N, O, X, R, O.\n",
      "Assistant: Good. What's the smallest row you can read with your left eye?\n",
      "Patient__: s e b m o\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here is the Assistant's response:\n",
      "\n",
      "**Assistant:** \"Good. I've made the necessary adjustments to the equipment. Your eyes are closed. I've put in your current prescription. Would you like me to read the smallest row you can see with both eyes, without squinting? Please let me know if they're closed or not.\"\n",
      "\n",
      "**Patient:** \"Yes, they are closed.\"\n",
      "\n",
      "**Assistant:** \"Okay. You can open your eyes. I'\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Thank you. Now close your eyes again for me. Are they closed?\n",
      "\n",
      "Similarity Score: [[0.457]]_______\n",
      "____________________\n",
      "elapsed time: 3.666 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: yes\n",
      "Assistant: Okay, open your eyes. I put in your current prescription. When you're ready, read the smallest row you can, ok?\n",
      "Patient__: O Z R S N.\n",
      "Assistant: Perfect. Try again with the right eye.\n",
      "Patient__: N O K H D.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here is the Assistant's response:\n",
      "\n",
      "**Assistant:** \"Okay, open your eyes. I've put in your current prescription. When you're ready, read the smallest row you can, ok?\"\n",
      "\n",
      "**Patient:** \"O Z R S N.\"\n",
      "\n",
      "**Assistant:** \"Perfect. Try again with the right eye. Please read the smallest row you can.\"\n",
      "\n",
      "**Patient:** \"N O K H D.\"\n",
      "\n",
      "**Assistant:** \"Thank you, you're\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Try again here with the left eye.\n",
      "\n",
      "Similarity Score: [[0.24]]_______\n",
      "____________________\n",
      "elapsed time: 3.702 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: R D O S N.\n",
      "Assistant: Okay, good. I'll uncover both eyes and you'll blink a few times. When you're ready, read the smallest line you can.\n",
      "Patient__: F C R V N.\n",
      "Assistant: Last comparison! Blink a few times. Is it better with number one? Or with number two?\n",
      "Patient__: two\n",
      "Assistant: OK, great. Choice one is where we started. Choice two is the new prescription. Does everything look clear to you?\n",
      "Patient__: yes that's a huge difference!\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "**Assistant:** \"Hello, R D O S N. I've got you in the exam room. Let's begin. I'm going to uncover both of your eyes and you're going to blink a few times. When you're ready, read the smallest line you can see on the chart. Please read the line out loud.\n",
      "\n",
      "**Patient:** \"F C R V N.\"\n",
      "\n",
      "**Assistant:** \"Last comparison! Blink a few times. Is it better with\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: I'm glad to hear that! So you're all set for distance. What we'll check next is your near vision. [LOCALTECH] will place a card in front of you. Let me know when the card is in place.\n",
      "\n",
      "Similarity Score: [[0.248]]_______\n",
      "____________________\n",
      "elapsed time: 3.809 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: They're about the same.\n",
      "Assistant: Perfect. Blink a few times and when you're ready, read the smallest line you can with your left eye.\n",
      "Patient__: ZZRSN.\n",
      "Assistant: Perfect. I'll uncover both eyes now. Blink again. When you're ready, read the last row.\n",
      "Patient__: NCKHD.\n",
      "Assistant: Last comparison! Blink a few times. Is it better with number one? Or with number two?\n",
      "Patient__: two\n",
      "Assistant: OK, great. Choice one is where we started. Choice two is the new prescription. Does everything look clear to you?\n",
      "Patient__: oh yes\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here is the Assistant's response:\n",
      "\n",
      "**Assistant:** \"Okay, I understand. You said your vision is about the same. Perfect. I'm going to uncover both eyes now. Would you like me to read the last row of the chart with your left eye first, or your right eye first? Please let me know.\"\n",
      "\n",
      "**Additional notes:**\n",
      "\n",
      "- The Assistant should be friendly and professional.\n",
      "- The Assistant should explain each step clearly and concisely.\n",
      "-\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Okay, good. So you're all set for distance. What we'll check next is your near vision. [LOCALTECH] will place a card in front of you. Let me know when the card is in place.\n",
      "\n",
      "Similarity Score: [[0.345]]_______\n",
      "____________________\n",
      "elapsed time: 3.726 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Assistant: Great. Now I will show you choices again, but with colors in the background. Does it look more clear with red? With green? Or are they the same?\n",
      "Patient__: red\n",
      "Assistant: Is it better on the red side, green side, or are they about the same?\n",
      "Patient__: they're both about the same now\n",
      "Assistant: Okay. Blink a few times. Read the smallest row you can with your right eye.\n",
      "Patient__: ZHCSR.\n",
      "Assistant: OK, now I'll test your left eye and we'll start again. Is it better with one? With two? Or are they the same?\n",
      "Patient__: two\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "**Assistant:** Sure, [Patient's name], I understand you're saying the red text looks clear. I'm going to show you the same choices again, but this time with some color in the background. Would you like me to show you the text with red, green, or are they the same?\n",
      "\n",
      "**Additional Notes:**\n",
      "- The Assistant should be friendly and professional.\n",
      "- The Assistant should explain each step of the exam clearly and concisely.\n",
      "- The Assistant\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Great! Let's compare the dots again. Is it better with one? Two? Or are they the same?\n",
      "\n",
      "Similarity Score: [[0.176]]_______\n",
      "____________________\n",
      "elapsed time: 3.690 seconds\n",
      "Prompt:\n",
      "### Instruction:\n",
      "You are an Optician conducting a vision exam. Use the dialogue below to create an Assistant response that guides the Patient__ through a vision exam.\n",
      "\n",
      "### Input:\n",
      "Patient__: h z c k o\n",
      "Assistant: Well done! I'll uncover both eyes now. Blink again. When you're ready, read the last row.\n",
      "Patient__: goube\n",
      "Assistant: Last comparison! Blink a few times. Is it better with number one? Or with number two?\n",
      "Patient__: number two\n",
      "Assistant: OK, great. Choice one is where we started. Choice two is the new prescription. Does everything look clear to you?\n",
      "Patient__: it does\n",
      "Assistant: Okay, good. So you're all set for distance. What we'll check next is the near vision. [LOCALTECH] will place a card in front of you. Let me know when the card is in place.\n",
      "LocalTech: The card is there.\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "____________________\n",
      "Generated response:\n",
      "Sure, here is the Assistant's response:\n",
      "\n",
      "**Assistant:** \"Alright, [Patient name], I've finished examining your distance vision. I'm going to move on to the near vision now. Please let me know when the card is in place. [LocalTech], please put the card in front of the patient.\"\n",
      "\n",
      "**Additional notes:**\n",
      "\n",
      "- The Assistant's tone is friendly and professional.\n",
      "- The Assistant clearly explains each step of the process.\n",
      "- The\n",
      "Ground truth:\n",
      "### Response:\n",
      "Assistant: Great! On this card, read the smallest line you can.\n",
      "\n",
      "Similarity Score: [[0.423]]_______\n",
      "____________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " OVERALL RESULTS\n",
      "Good predictions: 7\n",
      "Bad predictions:23\n",
      "Precision:0.23333333333333334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datasets import load_metric\n",
    "from contextlib import contextmanager\n",
    "from time import process_time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "@contextmanager\n",
    "def timer():\n",
    "    start = process_time()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        end = process_time()\n",
    "        elapsed = end - start\n",
    "        times.append(elapsed)\n",
    "        print(f'elapsed time: {elapsed:.3f} seconds')\n",
    "\n",
    "class SimilarityMetrics(object):\n",
    "    def __init__(self,model='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model)\n",
    "    def compute_similarity(self,pred, truth):\n",
    "        #Compute embedding for both lists\n",
    "        pred_embedding= self.model.encode(pred, convert_to_tensor=True)\n",
    "        truth_embedding = self.model.encode(truth, convert_to_tensor=True)\n",
    "        cos_sim = util.pytorch_cos_sim(pred_embedding, truth_embedding)\n",
    "        return cos_sim.cpu().numpy()\n",
    "    \n",
    "    def compute_bleu(pred:list, truth:list):\n",
    "        metric = load_metric('bleu')\n",
    "        # predictions (list of strs): Translations to score.\n",
    "        # references (list of lists of strs): references for each translation.\n",
    "        metric.add(predictions=pred, references=[truth])\n",
    "        report = metric.compute()\n",
    "        return report['bleu'] *100, report\n",
    "\n",
    "times = []\n",
    "tok_ps = []\n",
    "metrics = SimilarityMetrics()\n",
    "bad = 0\n",
    "good = 0\n",
    "\n",
    "if phoropterModel:\n",
    "    threshold = .96\n",
    "else:\n",
    "    threshold = .45\n",
    "big_test = test.data + val.data\n",
    "\n",
    "tests = 30\n",
    "\n",
    "if tests > len(big_test):\n",
    "    print(f\"Inadequate test set length: {len(big_test)}\")\n",
    "else:\n",
    "    sample_idxs = random.sample(range(0,len(big_test)), tests)\n",
    "\n",
    "for i in range(len(sample_idxs)):\n",
    "    sample = big_test[sample_idxs[i]]\n",
    "\n",
    "    prompt = format_inference(sample)\n",
    "    # prompt = format_mixtral(sample, model_input=False)\n",
    "    with timer():\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "        # with torch.inference_mode():\n",
    "        outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.3)\n",
    "        response = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
    "    tok_ps.append(outputs.detach().cpu().numpy().shape[1] / times[i])\n",
    "    result = metrics.compute_similarity(response[11:],sample[\"response\"][0][\"content\"])\n",
    "    if np.round(result, 3) < threshold:\n",
    "        bad += 1\n",
    "    else: good +=1\n",
    "    print(f\"Prompt:\\n{prompt}\\n\")\n",
    "    print('____________________')\n",
    "    print(f\"Generated response:\\n{response}\")\n",
    "    print(f\"Ground truth:\\n{format_inference(sample, model_input=False)}\")\n",
    "    print(f'Similarity Score: {np.round(result, 3)}_______')\n",
    "    print('____________________')\n",
    "print(f'\\n\\n\\n\\n OVERALL RESULTS\\nGood predictions: {good}\\nBad predictions:{bad}\\nPrecision:{good/(good+bad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference speed: 3.70 seconds\n",
      "Tokens / response: 68.00\n"
     ]
    }
   ],
   "source": [
    "def get_av(nums):\n",
    "    sum = 0\n",
    "    for i in nums:\n",
    "        sum += i\n",
    "    return sum / len(nums)\n",
    "\n",
    "print(f'Average inference speed: {get_av(times):.2f} seconds')\n",
    "print(f'Tokens / response: {get_av(tok_ps):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "strings =[]\n",
    "for i in test.data:\n",
    "    content = i[\"response\"][0][\"content\"]\n",
    "    if content not in strings:\n",
    "        strings.append(content)\n",
    "print(len(strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
